{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ComradeKommunist/TensorFlow-Lite-Final-Project/blob/main/Train_TFLite1_Object_Detection_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF8ysCfYKgTP"
      },
      "source": [
        "# TensorFlow Lite v1 Object Detection API in Colab\n",
        "**Author:** Evan Juras, [EJ Technology Consultants](https://ejtech.io)\n",
        "\n",
        "**Last updated:** 10/12/22\n",
        "\n",
        "**GitHub:** [TensorFlow Lite Object Detection](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi)\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This notebook implements [The TensorFlow Object Detection Library](https://github.com/tensorflow/models/tree/master/research/object_detection) for training an SSD-MobileNet model using your own dataset. Note that this notebook uses TensorFlow 1 rather than TensorFlow 2, because TensorFlow 1 works better for quantizing SSD-MobileNet models. If you want to use TensorFlow 2, [clink this link to go to the TensorFlow 2 version of this notebook](https://colab.research.google.com/github/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Train_TFLite2_Object_Detction_Model.ipynb).\n",
        "\n",
        "*Note: This notebook is not maintained or updated as frequently as the TF2 notebook.*\n",
        "\n",
        "I made a YouTube video that walks through the TensorFlow 2 notebook step by step, and the process is very similar for this TensorFlow 1 notebook. Please take a look at the video if you are confused about any steps in this notebook.\n",
        "\n",
        "*Link to video to be added here*\n",
        "\n",
        "###Working with Colab\n",
        "Simply click the play button on sections of code to execute them. As the code executes, any outputs will be displayed in a block beneath the code. Once they're done executing, a green checkmark will appear next to the section to indicate it's finished running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7EOtpvlLeS0"
      },
      "source": [
        "# 1. Install TensorFlow Object Detection Dependencies\n",
        "First, we'll install the TensorFlow Object Detection API in this Google Colab instance. This requires cloning the [TensorFlow models repository](https://github.com/tensorflow/models) and running a couple installation commands. Click the play button to run the following sections of code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QOFtpDlci50l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a8b0276-b831-435e-8a71-bb2c12dae723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Connected to cloud.r-pr\r                                                                               \rHit:2 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (108.157.173.52)] [C\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'libcuda-10.0-1' for regex 'cuda-10.0'\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Install CUDA 10.0 (needed for TF v1.15.3)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install cuda-10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KeoP1s8gi8tL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61805aec-c236-4499-8701-2bcb4abde037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-27 11:58:23--  https://www.dropbox.com/s/k6xqrje655q4aty/cudnn-10.0-linux-x64-v7.6.0.64.tgz\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/72ebodu3vepxeu7423nwh/cudnn-10.0-linux-x64-v7.6.0.64.tgz?rlkey=vx24s9uqqhpe371j36069g9zs [following]\n",
            "--2025-10-27 11:58:24--  https://www.dropbox.com/scl/fi/72ebodu3vepxeu7423nwh/cudnn-10.0-linux-x64-v7.6.0.64.tgz?rlkey=vx24s9uqqhpe371j36069g9zs\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca9b46328ef9fa3df60905481c8.dl.dropboxusercontent.com/cd/0/inline/C0BayymBwUoI9K35Bi6d2cbkOf2fo1p_6NvxeJLKxMZDIohhnyUk3GYm8fZOy08HjXotxhiT-CMEptq6KxAivLp4AVZc7l0IC-jZcWQxAEFb7NZMjEAHkScHJ-jfgXegjUKH294dY_y0dUzHWPEPLua5/file# [following]\n",
            "--2025-10-27 11:58:24--  https://uca9b46328ef9fa3df60905481c8.dl.dropboxusercontent.com/cd/0/inline/C0BayymBwUoI9K35Bi6d2cbkOf2fo1p_6NvxeJLKxMZDIohhnyUk3GYm8fZOy08HjXotxhiT-CMEptq6KxAivLp4AVZc7l0IC-jZcWQxAEFb7NZMjEAHkScHJ-jfgXegjUKH294dY_y0dUzHWPEPLua5/file\n",
            "Resolving uca9b46328ef9fa3df60905481c8.dl.dropboxusercontent.com (uca9b46328ef9fa3df60905481c8.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uca9b46328ef9fa3df60905481c8.dl.dropboxusercontent.com (uca9b46328ef9fa3df60905481c8.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/C0DAmiGrI0tE311GJC2nA8EOdLDmFcvYhZfqoiNYNtfA5jYx7W_ML9DetuB4CUmuNhRo-NHU3liVrBSU0JeM97jAfmVHo04_kDFyV1mak6HsnmwG1_M5AcsvoPA5I2VyFJpUKJyMAkRsbS_OuVChn5xJwvsudtOBH0F__2zdvr5zIUhErhwGhZ8iwqvQZGf365RdjiFK1Mp8Jri4uldmaSL8lH3Bbbm2wJsEV-cWnYfCOkAqwaVEbLloXEcb_PP5Rgw1cuKf9ReE_JgWLvaKP-lyUjAbmFe-cCLgyBYdlhg3hMz0BobXqER0d4zqpZtCNlN_BuOGAvU4_0p7vwhccve0k0WOKK7y2YjwMVwDkwprWiWBQ9wy_IXI7o0bRQFNI7I/file [following]\n",
            "--2025-10-27 11:58:25--  https://uca9b46328ef9fa3df60905481c8.dl.dropboxusercontent.com/cd/0/inline2/C0DAmiGrI0tE311GJC2nA8EOdLDmFcvYhZfqoiNYNtfA5jYx7W_ML9DetuB4CUmuNhRo-NHU3liVrBSU0JeM97jAfmVHo04_kDFyV1mak6HsnmwG1_M5AcsvoPA5I2VyFJpUKJyMAkRsbS_OuVChn5xJwvsudtOBH0F__2zdvr5zIUhErhwGhZ8iwqvQZGf365RdjiFK1Mp8Jri4uldmaSL8lH3Bbbm2wJsEV-cWnYfCOkAqwaVEbLloXEcb_PP5Rgw1cuKf9ReE_JgWLvaKP-lyUjAbmFe-cCLgyBYdlhg3hMz0BobXqER0d4zqpZtCNlN_BuOGAvU4_0p7vwhccve0k0WOKK7y2YjwMVwDkwprWiWBQ9wy_IXI7o0bRQFNI7I/file\n",
            "Reusing existing connection to uca9b46328ef9fa3df60905481c8.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 462318775 (441M) [application/x-gtar]\n",
            "Saving to: ‘cudnn-10.0-linux-x64-v7.6.0.64.tgz.1’\n",
            "\n",
            "cudnn-10.0-linux-x6 100%[===================>] 440.90M  94.1MB/s    in 4.4s    \n",
            "\n",
            "2025-10-27 11:58:29 (100 MB/s) - ‘cudnn-10.0-linux-x64-v7.6.0.64.tgz.1’ saved [462318775/462318775]\n",
            "\n",
            "cp: cannot create regular file '/usr/local/cuda-10.0/include': No such file or directory\n",
            "cp: target '/usr/local/cuda-10.0/lib64' is not a directory\n",
            "chmod: cannot access '/usr/local/cuda-10.0/include/cudnn*.h': No such file or directory\n",
            "chmod: cannot access '/usr/local/cuda-10.0/lib64/libcudnn*': No such file or directory\n",
            "cp: target '/usr/lib64-nvidia' is not a directory\n"
          ]
        }
      ],
      "source": [
        "# Install cuDNN 7.6.0 (needed for TF v1.15.3)\n",
        "# Download and extract cuDNN files\n",
        "!wget https://www.dropbox.com/s/k6xqrje655q4aty/cudnn-10.0-linux-x64-v7.6.0.64.tgz\n",
        "!tar -xf cudnn-10.0-linux-x64-v7.6.0.64.tgz\n",
        "\n",
        "# Copy cuDNN libraries to appropriate folders\n",
        "!sudo cp cuda/include/cudnn*.h /usr/local/cuda-10.0/include\n",
        "!sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-10.0/lib64\n",
        "!sudo chmod a+r /usr/local/cuda-10.0/include/cudnn*.h /usr/local/cuda-10.0/lib64/libcudnn*\n",
        "!sudo cp -P cuda/lib64/libcudnn* /usr/lib64-nvidia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ypWGYdPlLRUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b56d11-e748-44ea-dbd4-ba9a2f336a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Clone the tensorflow models repository from GitHub\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6QPmVBSlLTzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72201d6a-1632-41cb-bb29-b7f6b084bc39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/models/mymodel/models/research\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from object_detection==0.1) (11.3.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from object_detection==0.1) (5.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from object_detection==0.1) (3.10.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.12/dist-packages (from object_detection==0.1) (3.0.12)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.12/dist-packages (from object_detection==0.1) (21.6.0)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.12/dist-packages (from object_detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from object_detection==0.1) (1.17.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.12/dist-packages (from object_detection==0.1) (2.0.0)\n",
            "Requirement already satisfied: lvis in /usr/local/lib/python3.12/dist-packages (from object_detection==0.1) (0.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from object_detection==0.1) (1.16.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from object_detection==0.1) (2.2.2)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from lvis->object_detection==0.1) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from lvis->object_detection==0.1) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.12/dist-packages (from lvis->object_detection==0.1) (2.0.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.12/dist-packages (from lvis->object_detection==0.1) (4.12.0.88)\n",
            "Requirement already satisfied: pyparsing>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from lvis->object_detection==0.1) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from lvis->object_detection==0.1) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->object_detection==0.1) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->object_detection==0.1) (4.60.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->object_detection==0.1) (25.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->object_detection==0.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->object_detection==0.1) (2025.2)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from tf-slim->object_detection==0.1) (1.4.0)\n",
            "Building wheels for collected packages: object_detection\n",
            "  Building wheel for object_detection (setup.py): started\n",
            "  Building wheel for object_detection (setup.py): finished with status 'done'\n",
            "  Created wheel for object_detection: filename=object_detection-0.1-py3-none-any.whl size=1697261 sha256=1b2d4188aa004e0247c18844d10fd3ab4eca1f097852c835a0f0203b24b7b248\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-42_6rzyj/wheels/6d/1c/7d/63c4f44e6a159389507efe2114aaa4bfcb6df5c3f2329fa1ee\n",
            "Successfully built object_detection\n",
            "Installing collected packages: object_detection\n",
            "  Attempting uninstall: object_detection\n",
            "    Found existing installation: object_detection 0.1\n",
            "    Uninstalling object_detection-0.1:\n",
            "      Successfully uninstalled object_detection-0.1\n",
            "Successfully installed object_detection-0.1\n"
          ]
        }
      ],
      "source": [
        "# Install the Object Detection API\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf1/setup.py .\n",
        "python -m pip install --use-feature=2020-resolver ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH3fwht2w17a"
      },
      "source": [
        "If you get any errors running the following code block, you can ignore them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "kDT0ctlBlXTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cefaab59-d717-4850-813c-ced3db439ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.15.3 (from versions: 2.12.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-gpu==1.15.3\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting numpy==1.17.4\n",
            "  Using cached numpy-1.17.4.zip (6.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: numpy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for numpy (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for numpy\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for numpy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py clean\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[31m  ERROR: Failed cleaning build dir for numpy\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build numpy\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (numpy)\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pycocotools==2.0.0 in /usr/local/lib/python3.12/dist-packages (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "# Install TensorFlow\n",
        "!pip install tensorflow-gpu==1.15.3\n",
        "\n",
        "# A couple other packages have to be downgraded to work with the TF1 API\n",
        "!pip install numpy==1.17.4\n",
        "!pip install pycocotools==2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "wh_HPMOqWH9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "984b7924-8f79-4228-f609-3ab776364945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-27 12:00:32.217603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761566432.239271   12276 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761566432.245647   12276 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761566432.261647   12276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566432.261694   12276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566432.261698   12276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566432.261702   12276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/models/research/object_detection/builders/model_builder_tf1_test.py\", line 20, in <module>\n",
            "    from object_detection.builders import model_builder\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/object_detection/builders/model_builder.py\", line 26, in <module>\n",
            "    from object_detection.builders import hyperparams_builder\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/object_detection/builders/hyperparams_builder.py\", line 27, in <module>\n",
            "    from object_detection.core import freezable_sync_batch_norm\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/object_detection/core/freezable_sync_batch_norm.py\", line 20, in <module>\n",
            "    class FreezableSyncBatchNorm(tf.keras.layers.experimental.SyncBatchNormalization\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: module 'keras._tf_keras.keras.layers' has no attribute 'experimental'\n"
          ]
        }
      ],
      "source": [
        "# Run Model Bulider Test file, just to verify everything's working properly\n",
        "!python /content/models/research/object_detection/builders/model_builder_tf1_test.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPbU4I7aL9Fl"
      },
      "source": [
        "# 2. Upload Image Dataset and Prepare Training Data\n",
        "\n",
        "In this section, we'll upload our training images and use TFRecord generation scripts to prepare the training data for TensorFlow. we'll upload our images, split them into train, validation, and test folders, and then run scripts for creating TFRecords from our data.\n",
        "\n",
        "First, on your local PC, zip all your training images and XML files into a single folder called \"images.zip\". The files should be directly inside the zip folder (wihout any additional nested folders) as shown below:\n",
        "```\n",
        "images.zip\n",
        "-- img1.jpg\n",
        "-- img1.xml\n",
        "-- img2.jpg\n",
        "-- img2.xml\n",
        "...\n",
        "```\n",
        "There are two options for moving the image files to this Colab instance: you can upload them directly, or you can copy them from your Google Drive. If you have a slow internet connection or more than 50MB worth of images, I recommend using Google Drive. Otherwise, you can just upload them through Colab.\n",
        "\n",
        "#### Option 1. Upload through Google Colab\n",
        "Upload the \"images.zip\" file to the Google Colab instance by clicking the \"Files\" icon on the left hand side of the browser, and then the \"Upload to session storage\" icon. Select the zip folder to upload it.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Once it's uploaded, we'll run a few commands to unzip it and set up our image directories. These directories are created in the /content folder in this instance's filesystem. You can browse the filesystem by clicking the \"Files\" icon on the left.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpi58u0-V_jN"
      },
      "source": [
        "#### Option 2. Copy from Google Drive\n",
        "You can also upload your images to your personal Google Drive, mount the drive on this Colab session, and copy them over to the Colab filesystem. This option works well if you want to upload the images beforehand so you don't have to wait for them to upload each time you restart this Colab.\n",
        "\n",
        "First, upload the \"images.zip\" file to your Google Drive, and make note of the folder you uploaded them to. Replace `MyDrive/path/to/images.zip` with the path to your zip file. (For example, I uploaded the zip file to folder called \"cat-toys1\", so I would use `MyDrive/cat-toys1/images.zip` for the path). Then, run the following block of code to mount your Google Drive to this Colab session and copy the folder to this filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLgAPsQsfTLs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp /content/gdrive/MyDrive/path/to/images.zip /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLIu4GdjxgWu"
      },
      "source": [
        "#### Option 3. Use my bird, squirrel, raccoon dataset\n",
        "I've uploaded a dataset containing 800 labeled images of birds, squirrels, and raccoons. You can use this dataset if you just want to work through the process of training a TFLite model on custom data. Run the following code block to download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH6huqCBxhFb"
      },
      "outputs": [],
      "source": [
        "!wget -O /content/images.zip https://www.dropbox.com/s/en33x280e4z3wbt/BSR2.zip?dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-eXEQICx2ug"
      },
      "source": [
        "## Split images into train, validation, and test folders\n",
        "At this point, whether you used Option 1, 2, or 3, you should be able to click the folder icon on the left and see your \"images.zip\" file in the list of files. Now that the dataset is uploaded, let's unzip it and create some folders to hold the images. These directories are created in the /content folder in this instance's filesystem. You can browse the filesystem by clicking the \"Files\" icon on the left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "3n-E68MJbU2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3be2e71-fb00-44c5-b57e-5048ed46e67a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: images.zip not found. Please upload your dataset or use one of the provided options.\n"
          ]
        }
      ],
      "source": [
        "# Remove existing images directory to ensure a clean start\n",
        "!rm -rf /content/images\n",
        "\n",
        "# Create necessary directories\n",
        "!mkdir /content/images\n",
        "!mkdir /content/images/all\n",
        "!mkdir /content/images/train\n",
        "!mkdir /content/images/validation\n",
        "!mkdir /content/images/test\n",
        "\n",
        "# Check if images.zip exists before attempting to unzip\n",
        "import os\n",
        "if os.path.exists('images.zip'):\n",
        "    !unzip -q images.zip -d /content/images/all\n",
        "else:\n",
        "    print(\"Error: images.zip not found. Please upload your dataset or use one of the provided options.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-6RIcrwbQMh"
      },
      "source": [
        "Next, we'll split the images into train, validation, and test sets. Here's what each set is used for:\n",
        "\n",
        "\n",
        "*   **Train**: These are the actual images used to train the model. In each step of training, a batch of images from the \"train\" set is passed into the neural network. The network predicts classes and locations of objects in the images. The training algorithm calculates the loss (i.e. how \"wrong\" the predictions were) and adjusts the network weights through backpropagation.\n",
        "\n",
        "\n",
        "*   **Validation**: Images from the \"validation\" set can be used by the training algorithm to check the progress of training and adjust hyperparameters (like learning rate). Unlike \"train\" images, these images are only used periodically during training (i.e. once every certain number of training steps).\n",
        "\n",
        "\n",
        "* **Test**: These images are never seen by the neural network during training. They are intended to be used by a human to perform final testing of the model to check how accurate the model is.\n",
        "\n",
        "I wrote a Python script to randomly move 80% of the images to the \"train\" folder, 10% to the \"validation\" folder, and 10% to the \"test\" folder. Click play on the following block to download the script and execute it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "_8V38Uk2yBUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac54463-42b4-4d72-bfb5-9dfcc50f0460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-27 12:03:18--  https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/train_val_test_split.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3100 (3.0K) [text/plain]\n",
            "Saving to: ‘train_val_test_split.py.1’\n",
            "\n",
            "\r          train_val   0%[                    ]       0  --.-KB/s               \rtrain_val_test_spli 100%[===================>]   3.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-27 12:03:18 (30.0 MB/s) - ‘train_val_test_split.py.1’ saved [3100/3100]\n",
            "\n",
            "Total images: 0\n",
            "Images moving to train: 0\n",
            "Images moving to validation: 0\n",
            "Images moving to test: 0\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/train_val_test_split.py\n",
        "!python train_val_test_split.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p--K1PJXEgNo"
      },
      "source": [
        "## Create TFRecords\n",
        "Finally, we need to convert the images into a data file format called TFRecords, which are used by TensorFlow for training. We'll use Python scripts to automatically convert the data into TFRecord format. Before running them, we need to define a labelmap for our classes.\n",
        "\n",
        "The code section below will create a \"labelmap.txt\" file that contains a list of classes. Replace the `class1`, `class2`, `class3` text with your own classes (for example, `bird`, `squirrel`, `raccoon`), adding a new line for each class. Then, click play to execute the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "_DE_r4MKY7ln"
      },
      "outputs": [],
      "source": [
        "### This creates a a \"labelmap.txt\" file with a list of classes the object detection model will detect.\n",
        "%%bash\n",
        "cat <<EOF >> /content/labelmap.txt\n",
        "headphones\n",
        "bottle\n",
        "pen\n",
        "wire\n",
        "motion sensor\n",
        "box\n",
        "power supply\n",
        "74hc595\n",
        "magnifying glass\n",
        "buzzer\n",
        "led\n",
        "resistor\n",
        "protoboard\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pa2VYhTIT1l"
      },
      "source": [
        "Download and run the data conversion scripts by clicking play on the following two sections of code. They will create TFRecord files for the train and validation datasets, as well as a `labelmap.pbtxt` file which contains the labelmap in a different format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "v3KHhgrpHved",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be17d897-2b6e-443a-da07-43748e167c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-27 12:03:22--  https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_csv.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1348 (1.3K) [text/plain]\n",
            "Saving to: ‘create_csv.py.1’\n",
            "\n",
            "\rcreate_csv.py.1       0%[                    ]       0  --.-KB/s               \rcreate_csv.py.1     100%[===================>]   1.32K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-27 12:03:22 (55.2 MB/s) - ‘create_csv.py.1’ saved [1348/1348]\n",
            "\n",
            "--2025-10-27 12:03:22--  https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_tfrecord.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4414 (4.3K) [text/plain]\n",
            "Saving to: ‘create_tfrecord.py.1’\n",
            "\n",
            "create_tfrecord.py. 100%[===================>]   4.31K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-27 12:03:22 (49.2 MB/s) - ‘create_tfrecord.py.1’ saved [4414/4414]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download data conversion scripts\n",
        "! wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_csv.py\n",
        "! wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_tfrecord.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "5tdDbTmHYwu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d56a3ee4-b173-47a0-bb0e-e6d13045f32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Successfully converted xml to csv.\n",
            "Successfully converted xml to csv.\n",
            "2025-10-27 12:03:26.069270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761566606.093096   13071 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761566606.100153   13071 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761566606.117456   13071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566606.117502   13071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566606.117507   13071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566606.117511   13071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-27 12:03:30.909635: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Successfully created the TFRecords: /content/train.tfrecord\n",
            "2025-10-27 12:03:33.808507: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761566613.831837   13107 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761566613.838788   13107 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761566613.856404   13107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566613.856460   13107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566613.856464   13107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566613.856468   13107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-27 12:03:39.318202: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Successfully created the TFRecords: /content/val.tfrecord\n"
          ]
        }
      ],
      "source": [
        "# Create CSV data files and TFRecord files\n",
        "# Ensure we are in the /content directory to correctly access the images and labelmap files\n",
        "%cd /content/\n",
        "\n",
        "!python3 create_csv.py\n",
        "!python3 create_tfrecord.py --csv_input=images/train_labels.csv --labelmap=labelmap.txt --image_dir=images/train --output_path=train.tfrecord\n",
        "!python3 create_tfrecord.py --csv_input=images/validation_labels.csv --labelmap=labelmap.txt --image_dir=images/validation --output_path=val.tfrecord"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XcSBULRzNZ_"
      },
      "source": [
        "We'll store the locations of the TFRecord and labelmap files as variables so we can reference them later in this Colab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "YUd2wtfrqedy"
      },
      "outputs": [],
      "source": [
        "train_record_fname = '/content/train.tfrecord'\n",
        "val_record_fname = '/content/val.tfrecord'\n",
        "label_map_pbtxt_fname = '/content/labelmap.pbtxt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2MAcgJ53STW"
      },
      "source": [
        "# 3. Set Up Training Configuration\n",
        "\n",
        "In this section, we'll set up an SSD-MobileNet model training configuration. We'll specifiy which pretrained TensorFlow model we want to use from the [TensorFlow 1 Object Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md). Each model also comes with a configuration file that points to file locations, sets training parameters (such as learning rate and total number of training steps), and more. We'll modify the configuration file for our custom training job.\n",
        "\n",
        "The first section of code lists out some models availabe in the TF1 Model Zoo and defines some filenames that will be used later to download the model and config file. This makes it easy to manage which model you're using and to add other models to the list later.\n",
        "\n",
        "Set the \"chosen_model\" variable to match the name of the model you'd like to train with. It's currently set to use the \"ssd-mobilenet-v1-quantized\" model. Then, click play on the next three sections to specify and download the pretrained model file and configuration file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "gN0EUEa3e5Un"
      },
      "outputs": [],
      "source": [
        "# Change the \"chosen_model\" variable to select one of the models listed below (from the TF1 object detection zoo)\n",
        "\n",
        "chosen_model = 'ssd-mobilenet-v2-quantized'\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    'ssd-mobilenet-v1-quantized': {\n",
        "        'model_name': 'ssd_mobilenet_v1_quantized_coco',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz',\n",
        "    },\n",
        "    'ssd-mobilenet-v2-quantized': {\n",
        "        'model_name': 'ssd_mobilenet_v2_quantized_coco',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_quantized_300x300_coco.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz',\n",
        "    }\n",
        "}\n",
        "\n",
        "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
        "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
        "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "kG4TmJUVrYQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78655392-b0bd-4cf6-b8dd-65ca70a0ffbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/models/mymodel/’: File exists\n",
            "/content/models/mymodel\n",
            "--2025-10-27 12:03:47--  http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 172.217.204.207, 172.217.203.207, 173.194.215.207, ...\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|172.217.204.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 144806142 (138M) [application/x-tar]\n",
            "Saving to: ‘ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz.5’\n",
            "\n",
            "ssd_mobilenet_v2_qu 100%[===================>] 138.10M   222MB/s    in 0.6s    \n",
            "\n",
            "2025-10-27 12:03:48 (222 MB/s) - ‘ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz.5’ saved [144806142/144806142]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3625659931.py:11: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-27 12:03:51--  https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/samples/configs/ssd_mobilenet_v2_quantized_300x300_coco.config\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4823 (4.7K) [text/plain]\n",
            "Saving to: ‘ssd_mobilenet_v2_quantized_300x300_coco.config.5’\n",
            "\n",
            "\r          ssd_mobil   0%[                    ]       0  --.-KB/s               \rssd_mobilenet_v2_qu 100%[===================>]   4.71K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-27 12:03:51 (34.2 MB/s) - ‘ssd_mobilenet_v2_quantized_300x300_coco.config.5’ saved [4823/4823]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create \"mymodel\" folder for holding pre-trained weights and configuration files\n",
        "%mkdir /content/models/mymodel/\n",
        "%cd /content/models/mymodel/\n",
        "\n",
        "# Download pretrained model file from TensorFlow Model Zoo\n",
        "import tarfile\n",
        "download_tar = 'http://download.tensorflow.org/models/object_detection/' + pretrained_checkpoint\n",
        "\n",
        "!wget {download_tar}\n",
        "tar = tarfile.open(pretrained_checkpoint)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "# Download base training configuration file\n",
        "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/samples/configs/' + base_pipeline_file\n",
        "!wget {download_config}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jOYxQ20wXVr"
      },
      "source": [
        "Now that we've downloaded our model and config file, we need to modify the configuration file with some high-level training parameters. The following variables are used to control training steps:\n",
        "\n",
        "* **num_steps**: The total amount of steps to use for training the model. A good number to start with is 40,000 steps. You can use more steps if you notice the loss metrics are still decreasing by the time training finishes. The more steps, the longer the training. Training can also be stopped early if loss flattens out before reaching the specified number of steps.\n",
        "* **batch_size**: The number of images to use per training step. A larger batch size allows a model to be trained in fewer steps, but the size is limited by the GPU memory available for training. For the GPUs used in Colab instances, 16 is typically a good number.\n",
        "\n",
        "* **quant_delay_steps**: (Only used for quantization-aware training) After this many steps, the training algorithm will insert \"fake\" quantization nodes in the network to simulate the effects of quantization. A good starting point is half the total number of training steps. More information is available [here](https://neuralet.com/article/quantization-of-tensorflow-object-detection-api-models/).\n",
        "\n",
        "Other training information, like the location of the pretrained model file, the config file, and total number of classes are also assigned in this step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "IKiXz33QwXyP"
      },
      "outputs": [],
      "source": [
        "# Set training parameters for the model\n",
        "num_steps = 30000\n",
        "batch_size = 16\n",
        "quant_delay_steps = 15000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "b_ki9jOqxn7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962b7f36-2e38-44c3-988a-7e0bd547ac0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total classes: 52\n"
          ]
        }
      ],
      "source": [
        "# Set file locations and get number of classes for config file\n",
        "pipeline_fname = '/content/models/mymodel/' + base_pipeline_file\n",
        "fine_tune_checkpoint = '/content/models/mymodel/' + model_name + '/model.ckpt'\n",
        "\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "print('Total classes:', num_classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxpOXUTx0E-n"
      },
      "source": [
        "Finally, we'll rewrite the config file to use the training parameters we just specified. The following section of code will automatically replace the necessary parameters in the downloaded .config file and save it as our custom \"pipeline_file.config\" file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "5eA5ht3_yukT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4409faed-56ec-44e9-fa78-82ee3d56f25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/mymodel\n",
            "writing custom configuration file\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Write custom configuration file by slotting our dataset, model checkpoint, and training parameters into the base pipeline file\n",
        "# TO DO: change that description\n",
        "\n",
        "import re\n",
        "\n",
        "%cd /content/models/mymodel\n",
        "print('writing custom configuration file')\n",
        "\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open('pipeline_file.config', 'w') as f:\n",
        "\n",
        "    # fine_tune_checkpoint\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "\n",
        "    # tfrecord files train and test.\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/mscoco_train.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/mscoco_val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n",
        "\n",
        "    # label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "\n",
        "    # Fine-tune checkpoint type\n",
        "    s = re.sub(\n",
        "        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
        "\n",
        "    # Quantization delay steps\n",
        "    s = re.sub(\n",
        "        'delay: 48000', 'delay: {}'.format(quant_delay_steps), s)\n",
        "\n",
        "    f.write(s)\n",
        "\n",
        "%cd /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "HEsOLOMHzBqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d28beca-8aa8-4bb8-d8a0-1b9537c03c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Quantized trained SSD with Mobilenet v2 on MSCOCO Dataset.\n",
            "# Users should configure the fine_tune_checkpoint field in the train config as\n",
            "# well as the label_map_path and input_path fields in the train_input_reader and\n",
            "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
            "# should be configured.\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    num_classes: 52\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      ssd_anchor_generator {\n",
            "        num_layers: 6\n",
            "        min_scale: 0.2\n",
            "        max_scale: 0.95\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        aspect_ratios: 3.0\n",
            "        aspect_ratios: 0.3333\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 300\n",
            "        width: 300\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      convolutional_box_predictor {\n",
            "        min_depth: 0\n",
            "        max_depth: 0\n",
            "        num_layers_before_predictor: 0\n",
            "        use_dropout: false\n",
            "        dropout_keep_probability: 0.8\n",
            "        kernel_size: 1\n",
            "        box_code_size: 4\n",
            "        apply_sigmoid_to_scores: false\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            truncated_normal_initializer {\n",
            "              stddev: 0.03\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            train: true,\n",
            "            scale: true,\n",
            "            center: true,\n",
            "            decay: 0.9997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v2'\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          train: true,\n",
            "          scale: true,\n",
            "          center: true,\n",
            "          decay: 0.9997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid {\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      hard_example_miner {\n",
            "        num_hard_examples: 3000\n",
            "        iou_threshold: 0.99\n",
            "        loss_type: CLASSIFICATION\n",
            "        max_negatives_per_positive: 3\n",
            "        min_negatives_per_image: 3\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  batch_size: 16\n",
            "  optimizer {\n",
            "    rms_prop_optimizer: {\n",
            "      learning_rate: {\n",
            "        exponential_decay_learning_rate {\n",
            "          initial_learning_rate: 0.004\n",
            "          decay_steps: 800720\n",
            "          decay_factor: 0.95\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "      decay: 0.9\n",
            "      epsilon: 1.0\n",
            "    }\n",
            "  }\n",
            "  fine_tune_checkpoint: \"/content/models/mymodel/ssd_mobilenet_v2_quantized_coco/model.ckpt\"\n",
            "  fine_tune_checkpoint_type:  \"detection\"\n",
            "  # Note: The below line limits the training process to 200K steps, which we\n",
            "  # empirically found to be sufficient enough to train the pets dataset. This\n",
            "  # effectively bypasses the learning rate schedule (the learning rate will\n",
            "  # never decay). Remove the below line to train indefinitely.\n",
            "  num_steps: 30000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    ssd_random_crop {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/train.tfrecord\"\n",
            "  }\n",
            "  label_map_path: \"/content/labelmap.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  num_examples: 8000\n",
            "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
            "  # Remove the below line to evaluate indefinitely.\n",
            "  max_evals: 10\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/val.tfrecord\"\n",
            "  }\n",
            "  label_map_path: \"/content/labelmap.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}\n",
            "\n",
            "graph_rewriter {\n",
            "  quantization {\n",
            "    delay: 15000\n",
            "    weight_bits: 8\n",
            "    activation_bits: 8\n",
            "  }\n",
            "}"
          ]
        }
      ],
      "source": [
        "# Display the custom configuration file's contents\n",
        "!cat /content/models/mymodel/pipeline_file.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "GMlaN3rs3zLe"
      },
      "outputs": [],
      "source": [
        "# Set the path to the custom config file, and the directory to store training checkpoints in\n",
        "pipeline_file = '/content/models/mymodel/pipeline_file.config'\n",
        "model_dir = '/content/training/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLe247WqxPHw"
      },
      "source": [
        "The next block modifies the training script to save checkpoints once every 1000 training steps. Change `num_eval_steps` if you'd like to save checkpoints more or less frequently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "oi_wWkJOnc-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be0e8d7a-5876-43f9-c635-c160701387a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/object_detection\n",
            "Modifying model_main.py\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Modify model_main.py to set evaluation and checkpoint interval\n",
        "num_eval_steps = 1000\n",
        "%cd /content/models/research/object_detection\n",
        "print('Modifying model_main.py')\n",
        "\n",
        "\n",
        "with open('model_main.py', 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "  # Set eval steps\n",
        "  data = data.replace('config = tf_estimator.RunConfig(model_dir=FLAGS.model_dir)',\n",
        "             'config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir, save_checkpoints_steps={})'.format(num_eval_steps))\n",
        "\n",
        "  f.close()\n",
        "\n",
        "!rm /content/models/research/object_detection/model_main.py\n",
        "\n",
        "with open('model_main.py', 'w') as f:\n",
        "  f.write(data)\n",
        "\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "7wD0tda5oqIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39651ffa-162f-4f12-df84-b47690335b10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Binary to run train and evaluation on object detection model.\"\"\"\n",
            "\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from absl import flags\n",
            "\n",
            "import tensorflow.compat.v1 as tf\n",
            "from tensorflow.compat.v1 import estimator as tf_estimator\n",
            "\n",
            "from object_detection import model_lib\n",
            "\n",
            "flags.DEFINE_string(\n",
            "    'model_dir', None, 'Path to output model directory '\n",
            "    'where event and checkpoint files will be written.')\n",
            "flags.DEFINE_string('pipeline_config_path', None, 'Path to pipeline config '\n",
            "                    'file.')\n",
            "flags.DEFINE_integer('num_train_steps', None, 'Number of train steps.')\n",
            "flags.DEFINE_boolean('eval_training_data', False,\n",
            "                     'If training data should be evaluated for this job. Note '\n",
            "                     'that one call only use this in eval-only mode, and '\n",
            "                     '`checkpoint_dir` must be supplied.')\n",
            "flags.DEFINE_integer('sample_1_of_n_eval_examples', 1, 'Will sample one of '\n",
            "                     'every n eval input examples, where n is provided.')\n",
            "flags.DEFINE_integer('sample_1_of_n_eval_on_train_examples', 5, 'Will sample '\n",
            "                     'one of every n train input examples for evaluation, '\n",
            "                     'where n is provided. This is only used if '\n",
            "                     '`eval_training_data` is True.')\n",
            "flags.DEFINE_string(\n",
            "    'checkpoint_dir', None, 'Path to directory holding a checkpoint.  If '\n",
            "    '`checkpoint_dir` is provided, this binary operates in eval-only mode, '\n",
            "    'writing resulting metrics to `model_dir`.')\n",
            "flags.DEFINE_boolean(\n",
            "    'run_once', False, 'If running in eval-only mode, whether to run just '\n",
            "    'one round of eval vs running continuously (default).'\n",
            ")\n",
            "flags.DEFINE_integer(\n",
            "    'max_eval_retries', 0, 'If running continuous eval, the maximum number of '\n",
            "    'retries upon encountering tf.errors.InvalidArgumentError. If negative, '\n",
            "    'will always retry the evaluation.'\n",
            ")\n",
            "FLAGS = flags.FLAGS\n",
            "\n",
            "\n",
            "def main(unused_argv):\n",
            "  flags.mark_flag_as_required('model_dir')\n",
            "  flags.mark_flag_as_required('pipeline_config_path')\n",
            "  config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir, save_checkpoints_steps=1000)\n",
            "\n",
            "  train_and_eval_dict = model_lib.create_estimator_and_inputs(\n",
            "      run_config=config,\n",
            "      pipeline_config_path=FLAGS.pipeline_config_path,\n",
            "      train_steps=FLAGS.num_train_steps,\n",
            "      sample_1_of_n_eval_examples=FLAGS.sample_1_of_n_eval_examples,\n",
            "      sample_1_of_n_eval_on_train_examples=(\n",
            "          FLAGS.sample_1_of_n_eval_on_train_examples))\n",
            "  estimator = train_and_eval_dict['estimator']\n",
            "  train_input_fn = train_and_eval_dict['train_input_fn']\n",
            "  eval_input_fns = train_and_eval_dict['eval_input_fns']\n",
            "  eval_on_train_input_fn = train_and_eval_dict['eval_on_train_input_fn']\n",
            "  predict_input_fn = train_and_eval_dict['predict_input_fn']\n",
            "  train_steps = train_and_eval_dict['train_steps']\n",
            "\n",
            "  if FLAGS.checkpoint_dir:\n",
            "    if FLAGS.eval_training_data:\n",
            "      name = 'training_data'\n",
            "      input_fn = eval_on_train_input_fn\n",
            "    else:\n",
            "      name = 'validation_data'\n",
            "      # The first eval input will be evaluated.\n",
            "      input_fn = eval_input_fns[0]\n",
            "    if FLAGS.run_once:\n",
            "      estimator.evaluate(input_fn,\n",
            "                         steps=None,\n",
            "                         checkpoint_path=tf.train.latest_checkpoint(\n",
            "                             FLAGS.checkpoint_dir))\n",
            "    else:\n",
            "      model_lib.continuous_eval(estimator, FLAGS.checkpoint_dir, input_fn,\n",
            "                                train_steps, name, FLAGS.max_eval_retries)\n",
            "  else:\n",
            "    train_spec, eval_specs = model_lib.create_train_and_eval_specs(\n",
            "        train_input_fn,\n",
            "        eval_input_fns,\n",
            "        eval_on_train_input_fn,\n",
            "        predict_input_fn,\n",
            "        train_steps,\n",
            "        eval_on_train_data=False)\n",
            "\n",
            "    # Currently only a single Eval Spec is allowed.\n",
            "    tf_estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "  tf.app.run()\n"
          ]
        }
      ],
      "source": [
        "!cat /content/models/research/object_detection/model_main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxPj_QV43qD5"
      },
      "source": [
        "# Train Custom TF1 Object Detector\n",
        "\n",
        "We're ready to train our object detection model! Model training is performed using the \"model_main_tf2.py\" script from the TF Object Detection API. We've already defined all the parameters and arguments used by `model_main_tf2.py` in previous sections of this Colab. Training will take anywhere from 2 to 6 hours, depending on the model, batch size, and number of training steps. Just click Play on the following block to begin training!\n",
        "\n",
        "*Note: It takes a few minutes for the program to display any training messages, because it only displays logs once every 100 steps. If it seems like nothing is happening, just wait a couple minutes.*\n",
        "\n",
        "*Another note: If you want to stop training early, just click on the code block while it's running and press Ctrl+M to interrupt execution. You might have to press Ctrl+M a couple times and also click the Stop button or right-click and select \"Interrupt Execution\".*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "3mSabQWqvC2R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ef888f-e4ec-4243-ccc4-0889600d7301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboard==2.8.0\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.8.0) (1.75.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.8.0) (2.38.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.8.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.8.0) (3.9)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.8.0) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.8.0) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.8.0) (75.2.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard==2.8.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard==2.8.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.8.0) (3.1.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.8.0) (0.45.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.8.0) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.24.3->tensorboard==2.8.0) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=0.11.15->tensorboard==2.8.0) (3.0.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.8.0) (3.3.1)\n",
            "Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, tensorboard\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.2\n",
            "    Uninstalling google-auth-oauthlib-1.2.2:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires tensorboard~=2.19.0, but you have tensorboard 2.8.0 which is incompatible.\n",
            "pandas-gbq 0.29.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard==2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "TI9iCCxoNlAL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "b205a2db-44ab-484f-d02b-3cf497188bd1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with 1).\n",
              "Contents of stderr:\n",
              "Traceback (most recent call last):\n",
              "  File \"/usr/local/bin/tensorboard\", line 5, in <module>\n",
              "    from tensorboard.main import run_main\n",
              "  File \"/usr/local/lib/python3.12/dist-packages/tensorboard/main.py\", line 27, in <module>\n",
              "    from tensorboard import default\n",
              "  File \"/usr/local/lib/python3.12/dist-packages/tensorboard/default.py\", line 38, in <module>\n",
              "    from tensorboard.plugins.graph import graphs_plugin\n",
              "  File \"/usr/local/lib/python3.12/dist-packages/tensorboard/plugins/graph/graphs_plugin.py\", line 30, in <module>\n",
              "    from tensorboard.plugins.graph import keras_util\n",
              "  File \"/usr/local/lib/python3.12/dist-packages/tensorboard/plugins/graph/keras_util.py\", line 45, in <module>\n",
              "    from tensorboard.compat.tensorflow_stub import dtypes\n",
              "  File \"/usr/local/lib/python3.12/dist-packages/tensorboard/compat/tensorflow_stub/__init__.py\", line 22, in <module>\n",
              "    from .dtypes import as_dtype  # noqa\n",
              "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
              "  File \"/usr/local/lib/python3.12/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py\", line 326, in <module>\n",
              "    np.bool8: (False, True),\n",
              "    ^^^^^^^^\n",
              "  File \"/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\", line 410, in __getattr__\n",
              "    raise AttributeError(\"module {!r} has no attribute \"\n",
              "AttributeError: module 'numpy' has no attribute 'bool8'. Did you mean: 'bool'?"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/training'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "tQTfZChVzzpZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f82d0298-0b70-4225-ba92-84919902f20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-27 12:04:55.093014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761566695.236043   13490 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761566695.285452   13490 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761566695.400655   13490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566695.400964   13490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566695.400981   13490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566695.400989   13490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/models/research/object_detection/model_main.py\", line 24, in <module>\n",
            "    from tensorflow.compat.v1 import estimator as tf_estimator\n",
            "ImportError: cannot import name 'estimator' from 'tensorflow.compat.v1' (/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py)\n"
          ]
        }
      ],
      "source": [
        "!python /content/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path={pipeline_file} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vk2146Ogil3"
      },
      "source": [
        "# 5. Export Trained Inference Graph\n",
        "In the left sidebar, click the folder icon to view the files in the `/content` folder. Click the `training` folder to expand it, and find the checkpoint with the highest number (e.g. `model.ckpt-29000`). Replace \"XXXX\" in the code section below with that number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "YnSEZIzl4M10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d199deb-809a-492e-e26c-2a3d012ae5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/training/model.ckpt-30000\n",
            "2025-10-27 12:05:07.747273: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761566707.782140   13548 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761566707.792637   13548 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761566707.817164   13548 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566707.817218   13548 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566707.817224   13548 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761566707.817234   13548 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/models/research/object_detection/export_tflite_ssd_graph.py\", line 96, in <module>\n",
            "    from object_detection import export_tflite_ssd_graph_lib\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/object_detection/export_tflite_ssd_graph_lib.py\", line 26, in <module>\n",
            "    from object_detection import exporter\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/object_detection/exporter.py\", line 24, in <module>\n",
            "    from object_detection.builders import model_builder\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/object_detection/builders/model_builder.py\", line 26, in <module>\n",
            "    from object_detection.builders import hyperparams_builder\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/object_detection/builders/hyperparams_builder.py\", line 27, in <module>\n",
            "    from object_detection.core import freezable_sync_batch_norm\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/object_detection/core/freezable_sync_batch_norm.py\", line 20, in <module>\n",
            "    class FreezableSyncBatchNorm(tf.keras.layers.experimental.SyncBatchNormalization\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: module 'keras._tf_keras.keras.layers' has no attribute 'experimental'\n"
          ]
        }
      ],
      "source": [
        "!mkdir /content/fine_tuned_model_lite\n",
        "output_directory = '/content/fine_tuned_model_lite'\n",
        "\n",
        "# Replace \"XXXX\" in the line below with the latest checkpoint in the /content/training directory\n",
        "last_model_path = '/content/training/model.ckpt-30000'\n",
        "print(last_model_path)\n",
        "\n",
        "!python /content/models/research/object_detection/export_tflite_ssd_graph.py \\\n",
        "    --trained_checkpoint_prefix {last_model_path} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {pipeline_file}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwoGtqdejiez"
      },
      "source": [
        "# 6. Convert Model to TensorFlow Lite Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "PW7TpzLmhTbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a7a9e6-2e10-46cf-80b2-eda992cc314b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'normalized_input_image_tensor': [1, 300, 300, 3]}\n"
          ]
        }
      ],
      "source": [
        "localpb = '/content/fine_tuned_model_lite/tflite_graph.pb'\n",
        "tflite_file = '/content/fine_tuned_model_lite/detect.tflite'\n",
        "\n",
        "# Can use Netron app to determine the input size: https://www.electronjs.org/apps/netron\n",
        "input_size = [1, 300, 300, 3]\n",
        "input_shape = {\"normalized_input_image_tensor\" : input_size}\n",
        "\n",
        "print(input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "Ec8LzJLbr-SR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "570b79c9-0658-4f9c-9487-d5d8e5176b3e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "type object 'TFLiteConverterV2' has no attribute 'from_frozen_graph'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3666865353.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m converter = tf.lite.TFLiteConverter.from_frozen_graph(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlocalpb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m\"normalized_input_image_tensor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'TFLiteConverterV2' has no attribute 'from_frozen_graph'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_frozen_graph(\n",
        "    localpb,\n",
        "    [\"normalized_input_image_tensor\"],\n",
        "    [\"TFLite_Detection_PostProcess\",\"TFLite_Detection_PostProcess:1\",\"TFLite_Detection_PostProcess:2\",\"TFLite_Detection_PostProcess:3\"],\n",
        "    input_shape\n",
        ")\n",
        "\n",
        "converter.allow_custom_ops = True\n",
        "converter.inference_type = tf.uint8  # Tell converter to create a quantized model\n",
        "converter.quantized_input_stats = {'normalized_input_image_tensor': (128, 128)}\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "open(tflite_file,'wb').write(tflite_model)\n",
        "\n",
        "# Test that the conversion was successful - if any errors are generated, something went wrong!\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awZMQGVqMpVL"
      },
      "outputs": [],
      "source": [
        "!cp /content/labelmap.txt /content/fine_tuned_model_lite\n",
        "!cp /content/labelmap.pbtxt /content/fine_tuned_model_lite\n",
        "!zip -r /content/fine_tuned_model_lite.zip /content/fine_tuned_model_lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w41-KMn7KV-T"
      },
      "source": [
        "Now, you can download the \"detect.tflite\" and \"labelmap.txt\" files and move them to your Raspberry Pi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQyjuMAt1WEL"
      },
      "source": [
        "# (Optional) Test out TensorFlow Lite Model\n",
        "Now that we've got our custom detection model trained and converted to TFLite format, let's try it out on some images. We'll use the images from our \"test\" folder. These images weren't seen at all by the model during training, so they'll give a faithful depiction of how accurate the model actually is.\n",
        "\n",
        "The following code is used to import the TensorFlow Lite runtime, load it into memory, run inferencing on the test images, and display the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1IBMfrD2GEQ"
      },
      "outputs": [],
      "source": [
        "# Copied from my TFLite repository, and stripped unneeded stuff out\n",
        "\n",
        "# Import packages\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import glob\n",
        "import importlib.util\n",
        "from tensorflow.lite.python.interpreter import Interpreter\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "PATH_TO_IMAGES='/content/images/test'\n",
        "PATH_TO_MODEL='/content/fine_tuned_model_lite/detect.tflite'\n",
        "PATH_TO_LABELS='/content/labelmap.txt'\n",
        "IMAGES_TO_TEST = 10\n",
        "min_conf_threshold=0.5\n",
        "\n",
        "# Grab all image filenames\n",
        "images = glob.glob(PATH_TO_IMAGES + '/*.jpg')\n",
        "images = images + glob.glob(PATH_TO_IMAGES + '/*.JPG')\n",
        "images = images + glob.glob(PATH_TO_IMAGES + '/*.png')\n",
        "\n",
        "\n",
        "# Load the label map into memory\n",
        "with open(PATH_TO_LABELS, 'r') as f:\n",
        "    labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "\n",
        "# Load the Tensorflow Lite model into memory\n",
        "interpreter = Interpreter(model_path=PATH_TO_MODEL)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get model details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "height = input_details[0]['shape'][1]\n",
        "width = input_details[0]['shape'][2]\n",
        "\n",
        "floating_model = (input_details[0]['dtype'] == np.float32)\n",
        "\n",
        "input_mean = 127.5\n",
        "input_std = 127.5\n",
        "\n",
        "# Loop over every image and perform detection\n",
        "for image_path in images[0:IMAGES_TO_TEST]:\n",
        "\n",
        "    # Load image and resize to expected shape [1xHxWx3]\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    imH, imW, _ = image.shape\n",
        "    image_resized = cv2.resize(image_rgb, (width, height))\n",
        "    input_data = np.expand_dims(image_resized, axis=0)\n",
        "\n",
        "    # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
        "    if floating_model:\n",
        "        input_data = (np.float32(input_data) - input_mean) / input_std\n",
        "\n",
        "    # Perform the actual detection by running the model with the image as input\n",
        "    interpreter.set_tensor(input_details[0]['index'],input_data)\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Retrieve detection results\n",
        "    boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\n",
        "    classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects\n",
        "    scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects\n",
        "    #num = interpreter.get_tensor(output_details[3]['index'])  # Total number of detected objects (inaccurate and not needed)\n",
        "\n",
        "    # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
        "    for i in range(len(scores)):\n",
        "        if ((scores[i] > min_conf_threshold) and (scores[i] <= 1.0)):\n",
        "\n",
        "            # Get bounding box coordinates and draw box\n",
        "            # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
        "            ymin = int(max(1,(boxes[i][0] * imH)))\n",
        "            xmin = int(max(1,(boxes[i][1] * imW)))\n",
        "            ymax = int(min(imH,(boxes[i][2] * imH)))\n",
        "            xmax = int(min(imW,(boxes[i][3] * imW)))\n",
        "\n",
        "            cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
        "\n",
        "            # Draw label\n",
        "            object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
        "            label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
        "            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
        "            label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
        "            cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
        "            cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
        "\n",
        "    # All the results have been drawn on the image, now display the image\n",
        "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    plt.figure(figsize=(12,16))\n",
        "    plt.imshow(image)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFsuasvxFHo8"
      },
      "source": [
        "## (Optional) Compile Model for Edge TPU\n",
        "\n",
        "Now that the model has been converted to TFLite and quantized, we can compile it to run on Edge TPU devices like the [Coral USB Accelerator](https://coral.ai/products/accelerator/) or the [Coral Dev Board](https://coral.ai/products/dev-board/). This allows the model to run much faster! For more information on the Edge TPU, see my [TensorFlow Lite repository on GitHub](fine_tuned_model_lite).\n",
        "\n",
        "First, install the Edge TPU Compiler package inside this Colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUd_SNC0JSq0"
      },
      "outputs": [],
      "source": [
        "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "! sudo apt-get update\n",
        "! sudo apt-get install edgetpu-compiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usfmdtSiJuuC"
      },
      "source": [
        "Next, compile the model. (If your model has a different filename than \"detect.tflite\", use that instead.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTboEAWuJ0ku"
      },
      "outputs": [],
      "source": [
        "%cd /content/fine_tuned_model_lite\n",
        "!edgetpu_compiler detect.tflite\n",
        "!mv detect_edgetpu.tflite edgetpu.tflite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqGy2FgzKomN"
      },
      "source": [
        "The compiled model will be output in the /content folder as \"detect_edgetpu.tflite\". It gets renamed to \"edgetpu.tflite\" to be consistent with my code. Download this file using the command below. (If your model has a different filename than \"edgetpu.tflite\", use that instead.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgvZC_y5ESYq"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!cp labelmap.txt fine_tuned_model_lite\n",
        "!cp labelmap.pbtxt fine_tuned_model_lite\n",
        "!zip -r fine_tuned_model_lite.zip fine_tuned_model_lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmjqvKuuK8ZR"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('edgetpu.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptwpBBEWLfuJ"
      },
      "source": [
        "Now you're all set to use the Coral model! For instructions on how to run an object detection model on the Raspberry Pi using the Coral USB Acclerator, please see my video, [\"How to Use the Coral USB Accelerator with the Raspberry Pi\"](https://www.youtube.com/watch?v=qJMwNHQNOVU)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}